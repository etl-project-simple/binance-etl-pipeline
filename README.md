# Binance Real-Time ETL Pipeline

## üìå Overview
This project implements an **end-to-end ETL pipeline** for real-time cryptocurrency trades (Binance).  
The system streams raw trades from the Binance WebSocket API into PostgreSQL, processes them daily with **PySpark**, stores clean data in **Amazon S3**, and loads dimension/fact tables into **Amazon Redshift** for analytics.  
The pipeline is fully orchestrated with **Apache Airflow**.

---

## üìê Architecture
![ETL Pipeline Diagram](etl_pipeline.png)

**Workflow:**
1. **Ingestion:** Binance WebSocket ‚Üí PostgreSQL (`trades` table).  
2. **Extraction:** PySpark reads data via JDBC.  
3. **Transformation:** Create dimension and fact tables (`dim_time`, `dim_symbol`, `fact_trades`).  
4. **Load:** Write Parquet to **Amazon S3** and COPY into **Amazon Redshift**.  
5. **Orchestration:** Daily scheduled ETL with Airflow at 04:00.  

---

## üìÇ Project Structure
```
.
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ etl_dags.py          # Airflow DAG to orchestrate ETL
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ extract.py           # Extract trades from PostgreSQL ‚Üí Spark
‚îÇ   ‚îú‚îÄ‚îÄ transform.py         # Transform into dim_time, dim_symbol, fact_trades
‚îÇ   ‚îú‚îÄ‚îÄ load.py              # Load Parquet to S3 + COPY into Redshift
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # Helper for JDBC connections
‚îÇ   ‚îî‚îÄ‚îÄ main.py              # Main ETL entrypoint
‚îú‚îÄ‚îÄ stream.py                # Binance WebSocket ‚Üí PostgreSQL (real-time ingestion)
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile               # Container image for Airflow/Spark
‚îú‚îÄ‚îÄ docker-compose.yaml      # Local orchestration (Airflow, PostgreSQL, etc.)
‚îú‚îÄ‚îÄ airflow.cfg              # Airflow configs
‚îú‚îÄ‚îÄ .env                     # Environment variables (AWS, Redshift, Postgres)
```

---

## ‚öôÔ∏è Pipeline Architecture
```mermaid
flowchart TD
    A[Binance WebSocket] -->|Stream| B[(PostgreSQL: trades)]
    B -->|Extract via JDBC| C[PySpark]
    C -->|Transform| D[dim_time, dim_symbol, fact_trades]
    D -->|Write Parquet| E[(Amazon S3: data_clean)]
    E -->|COPY| F[(Amazon Redshift)]
    F -->|BI Tools| G[Analytics & Dashboards]
```

---

## üöÄ Features
- **Streaming ingestion** (`stream.py`) from Binance WebSocket into PostgreSQL.
- **Daily batch ETL** with PySpark:
  - `dim_time`: timestamp dimension
  - `dim_symbol`: distinct symbols
  - `fact_trades`: aggregated trade facts (min, max, avg price, volume, value).
- **Data Lake** on S3 (partitioned by `year/month/day`).
- **Data Warehouse** on Redshift with COPY from S3.
- **Orchestration** using Apache Airflow (DAG scheduled daily at 04:00).

---

## üõ†Ô∏è Installation & Setup

### 1. Clone repository
```bash
git clone https://github.com/<your-username>/binance-etl.git
cd binance-etl
```

### 2. Create .env file (based on .env.simple)
```bash
cp .env.example .env
```

### 3. Build & start services
```bash
docker-compose up -d --build
```

- Airflow UI ‚Üí [http://localhost:8080](http://localhost:8080)  
- PostgreSQL ‚Üí `localhost:5432`  

---

## üìä Demo Results

After a daily run, transformed tables are written:

‚úÖ **S3 (Parquet):**
```
s3://binance81/data_clean/dim_time/year=2025/month=09/day=10/
s3://binance81/data_clean/dim_symbol/year=2025/month=09/day=10/
s3://binance81/data_clean/fact_trades/year=2025/month=09/day=10/
```

‚úÖ **Redshift Tables:**
```sql
SELECT COUNT(*) FROM public.fact_trades;   -- Fact table
SELECT * FROM public.dim_symbol LIMIT 5;   -- Dimension table
```

---

## üßæ Example Output

### 1. `dim_time`
| datetime            | year | month | day | hour | minute |
|---------------------|------|-------|-----|------|--------|
| 2025-09-10 04:00:00 | 2025 | 9     | 10  | 4    | 0      |
| 2025-09-10 04:01:00 | 2025 | 9     | 10  | 4    | 1      |

---

### 2. `dim_symbol`
| symbol |
|--------|
| BTCUSDT|
| ETHUSDT|

---

### 3. `fact_trades`
| symbol  | trade_minute       | min_price | max_price | avg_price | total_quantity | total_trade_value |
|---------|--------------------|-----------|-----------|-----------|----------------|-------------------|
| BTCUSDT | 2025-09-10 04:00:00| 57000.10  | 57005.50  | 57002.80  | 1.52           | 86763.21          |
| BTCUSDT | 2025-09-10 04:01:00| 57006.00  | 57020.75  | 57012.90  | 0.85           | 48460.96          |
| ETHUSDT | 2025-09-10 04:00:00| 2200.30   | 2205.70   | 2203.25   | 12.50          | 27540.62          |

---

## üì¶ Dependencies
See [`requirements.txt`](requirements.txt):
- PySpark 3.5.1
- pandas, pyarrow
- psycopg2-binary
- boto3 / botocore
- apache-airflow-providers-{amazon,spark}

---

## üìÖ Airflow DAG
- DAG ID: **daily_binance_etl**
- Schedule: `0 4 * * *` (04:00 daily)
- Task: `run_etl_pipeline`

---

## üìà Future Improvements
- Multi-symbol streaming (BTC, ETH, ‚Ä¶).
- Near-real-time micro-batch Spark Structured Streaming.
- Data quality checks before Redshift load.
- Dashboard integration with Amazon QuickSight or Power BI.
